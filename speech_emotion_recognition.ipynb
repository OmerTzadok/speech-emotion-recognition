{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe9d62f9-500f-42a4-be40-481e2b1bfcd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16245d3-8a8a-4d97-b1e4-7845ac5a789f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "import soundfile as sf\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae651683-fc37-4077-8fed-f076fa8cdcda",
   "metadata": {},
   "source": [
    "# 2. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7782d9d5-b4d1-4460-a716-1ddda02e5b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAVDESS = \"./data/ravdess-emotional-speech-audio/\"\n",
    "CREMA = \"./data/cremad/\"\n",
    "TESS = \"./data/toronto-emotional-speech-set-tess/\"\n",
    "SAVEE = \"./data/surrey-audiovisual-expressed-emotion-savee/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c58262",
   "metadata": {},
   "source": [
    "## 2.1. Ravdess dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ddabe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ravdess_actors_list = os.listdir(RAVDESS)\n",
    "audio_emotion = []\n",
    "audio_path = []\n",
    "audio_gender = []\n",
    "audio_intensity = []\n",
    "durations = []\n",
    "\n",
    "for file in ravdess_actors_list:\n",
    "    actor = os.listdir(os.path.join(RAVDESS,file))\n",
    "    for audio in actor:\n",
    "        list_audio = audio.split('.')[0] # splitting by the '.' into '**-**-**' . 'wav' and grab the first element\n",
    "        list_audio = list_audio.split('-') # splitting by the dash\n",
    "        gender_code = int(list_audio[6])\n",
    "        \n",
    "        audio_emotion.append(int(list_audio[2])) # the third element describes the emotion class\n",
    "        audio_gender.append('female' if gender_code & 1 == 0 else 'male') # the sixth element describes the voice gender \n",
    "        audio_intensity.append('normal' if int(list_audio[3]) == 1 else 'high')\n",
    "        audio_path.append(os.path.join(RAVDESS,file,audio))\n",
    "        \n",
    "        audio, sr = librosa.load(os.path.join(RAVDESS,file,audio))\n",
    "        durations.append(librosa.get_duration(y=audio, sr=sr))\n",
    "\n",
    "ravdess_df = pd.DataFrame({'emotions': audio_emotion, 'genders': audio_gender, 'audio_intensity': audio_intensity, 'audio_path': audio_path, 'duration':durations})\n",
    "\n",
    "# Mapping the values to emotions\n",
    "emotion_dict = {\n",
    "    1: 'neutral',\n",
    "    2: 'neutral', # originally - - > 'calm'\n",
    "    3: 'happy',\n",
    "    4: 'sad',\n",
    "    5: 'angry',\n",
    "    6: 'fear',\n",
    "    7: 'disgust',\n",
    "    8: 'surprised',\n",
    "}\n",
    "\n",
    "# Replace the values in the column with emotions\n",
    "ravdess_df['emotions'] = ravdess_df['emotions'] .replace(emotion_dict)\n",
    "ravdess_df['dataset'] = 'ravdess'\n",
    "ravdess_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fdd96c",
   "metadata": {},
   "source": [
    "## 2.2. Crema dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd16ae41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crema_list = os.listdir(CREMA)\n",
    "audio_emotion = []\n",
    "audio_path = []\n",
    "audio_intensity = []\n",
    "gender_list = []\n",
    "durations = []\n",
    "\n",
    "female_id_list = [\n",
    "    '1002', '1003', '1004', '1006', '1007', '1008', '1009', '1010', '1012', '1013', '1018', \n",
    "    '1020', '1021', '1024', '1025', '1028', '1029', '1030', '1037', '1043', '1046', '1047', \n",
    "    '1049', '1052', '1053', '1054', '1055', '1056', '1058', '1060', '1061', '1063', '1072', \n",
    "    '1073', '1074', '1075', '1076', '1078', '1079', '1082', '1084', '1089', '1091',\n",
    "]\n",
    "\n",
    "emotion_dict = {\n",
    "    'HAP' : 'happy',\n",
    "    'NEU' : 'neutral',\n",
    "    'SAD' : 'sad',\n",
    "    'ANG' : 'angry',\n",
    "    'FEA' : 'fear',\n",
    "    'DIS' : 'disgust',\n",
    "}\n",
    "\n",
    "intensity_dict = {\n",
    "    'XX' : 'normal',\n",
    "    'X' : 'normal',\n",
    "    'LO' : 'low',\n",
    "    'MD' : 'normal',\n",
    "    'HI': 'high'\n",
    "}\n",
    "\n",
    "for audio in crema_list:\n",
    "    list_audio = audio.split('.')[0] # splitting by the '.' into '**-**-**' . 'wav' and grab the first element\n",
    "    list_audio = list_audio.split('_') # splitting by the underline\n",
    "    audio_emotion.append(list_audio[2])\n",
    "    audio_intensity.append(list_audio[3])\n",
    "    audio_path.append(os.path.join(CREMA,audio))\n",
    "    gender_list.append('female' if list_audio[0] in female_id_list else 'male')\n",
    "    \n",
    "    audio, sr = librosa.load(os.path.join(CREMA,audio))\n",
    "    durations.append(librosa.get_duration(y=audio, sr=sr))\n",
    "    \n",
    "crema_df = pd.DataFrame({'emotions': audio_emotion, 'genders': gender_list, 'audio_intensity': audio_intensity, 'audio_path': audio_path, 'duration':durations})\n",
    "crema_df['emotions'] = crema_df['emotions'].replace(emotion_dict)\n",
    "crema_df['audio_intensity'] = crema_df['audio_intensity'].replace(intensity_dict)\n",
    "crema_df['dataset'] = 'crema'\n",
    "crema_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0a6b1-22a9-40c4-b52d-21afe81c80d9",
   "metadata": {},
   "source": [
    "## 2.3. TESS dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e2bcb-99e0-4c90-8dbc-aad074ff9882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tess_dir_list = os.listdir(TESS)\n",
    "path_list = []\n",
    "gender_list = []\n",
    "emotion_list = [] \n",
    "audio_intensity = []\n",
    "durations = []\n",
    "\n",
    "emotion_dict = {\n",
    "    'happy'   : 'happy',\n",
    "    'neutral' : 'neutral',\n",
    "    'sad'     : 'sad',\n",
    "    'ps'     : 'surprised',\n",
    "    'angry'   : 'angry',\n",
    "    'fear'    : 'fear',\n",
    "    'disgust'  : 'disgust',\n",
    "}\n",
    "\n",
    "for directory in tess_dir_list:\n",
    "    audio_files = os.listdir(os.path.join(TESS, directory))\n",
    "    for audio_file in audio_files:\n",
    "        part = audio_file.split('.')[0]\n",
    "        path_list.append(os.path.join(TESS,directory,audio_file))\n",
    "        gender_list.append('female') # female only dataset\n",
    "        audio_intensity.append('normal') # normal only dataset\n",
    "        emotion_list.append(part.split('_')[2])\n",
    "        \n",
    "        audio, sr = librosa.load(os.path.join(TESS,directory,audio_file))\n",
    "        durations.append(librosa.get_duration(y=audio, sr=sr))\n",
    "            \n",
    "tess_df = pd.DataFrame({'emotions': emotion_list, 'genders': gender_list, 'audio_intensity': audio_intensity, 'audio_path': path_list, 'duration':durations})\n",
    "tess_df['emotions'] = tess_df['emotions'].replace(emotion_dict)\n",
    "tess_df['dataset'] = 'tess'\n",
    "tess_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbc0eef-7e88-48b6-a78a-73c97bcdd82e",
   "metadata": {},
   "source": [
    "## 2.3. SAVEE dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79812cdc-6f33-496a-a76a-1f5d8687ccaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "savee_dir_list = os.listdir(SAVEE)\n",
    "path_list = []\n",
    "gender_list = []\n",
    "emotion_list = []\n",
    "audio_intensity = []\n",
    "durations = []\n",
    "\n",
    "emotion_dict = {\n",
    "    'h'  : 'happy',\n",
    "    'n'  : 'neutral',\n",
    "    'sa' : 'sad',\n",
    "    'a'  : 'angry',\n",
    "    'f'  : 'fear',\n",
    "    'd'  : 'disgust',\n",
    "    'su' : 'surprised'\n",
    "}\n",
    "\n",
    "for audio_file in savee_dir_list:\n",
    "    part = audio_file.split('_')[1]\n",
    "    path_list.append(os.path.join(SAVEE,audio_file))\n",
    "    gender_list.append('male') # male only dataset\n",
    "    emotion_list.append(part[:-6])\n",
    "    audio_intensity.append('normal') # normal only dataset\n",
    "    \n",
    "    audio, sr = librosa.load(os.path.join(SAVEE,audio_file))\n",
    "    durations.append(librosa.get_duration(y=audio, sr=sr))\n",
    "        \n",
    "savee_df = pd.DataFrame({'emotions': emotion_list, 'genders': gender_list, 'audio_intensity': audio_intensity, 'audio_path': path_list,'duration':durations})\n",
    "savee_df['emotions'] = savee_df['emotions'].replace(emotion_dict)\n",
    "savee_df['dataset'] = 'savee'\n",
    "\n",
    "savee_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4eea19-f752-4a55-928d-48313195d15c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df = pd.concat([ravdess_df, crema_df, tess_df, savee_df], axis=0)\n",
    "merged_df.reset_index(inplace=True, drop=True)\n",
    "merged_df.to_csv('merged_df.csv',index=False)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f3bf9c-7f3a-46a3-9bd0-da850193229c",
   "metadata": {},
   "source": [
    "# 3. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa979fb",
   "metadata": {},
   "source": [
    "## 3.1. Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3937684",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "emotion_percent = merged_df['emotions'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Set a threshold for small percentage slices\n",
    "threshold = 5\n",
    "\n",
    "# Identify emotions below the threshold\n",
    "small_emotions = emotion_percent[emotion_percent < threshold]\n",
    "\n",
    "# Create a list of explode values\n",
    "explode = [0.1 if emotion in small_emotions else 0 for emotion in emotion_percent.index]\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define a custom color palette\n",
    "color_palette = [\"#0bb4ff\", \"#50e991\", \"#e6d800\", \"#9b19f5\", \"#e60049\"] \n",
    "\n",
    "# Create the pie chart with explode values\n",
    "plt.pie(emotion_percent, labels=emotion_percent.index, autopct='%1.1f%%', colors=color_palette, explode=explode)\n",
    "\n",
    "# Set the title\n",
    "plt.title('Distribution of Emotions', size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4724b559",
   "metadata": {},
   "source": [
    "The pie chart represents the distribution of emotion types in the dataset. <br>\n",
    "It is evident that apart from the surprised tone with 5.4% , the other types of tones are around 14-16%. <br>\n",
    "Worth mentioning that the tone 'calm' with its 1.6% distribution in the dataset, was merged with the neutral tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a739197-466f-4398-89bf-72b5f3a47f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of each emotion by gender\n",
    "emotions_by_gender = merged_df.groupby('genders')['emotions'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Reset the index to convert the result into a DataFrame\n",
    "emotions_by_gender = emotions_by_gender.reset_index(name='percentage')\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "sns.barplot(emotions_by_gender, x='emotions', y='percentage', hue='genders', palette='Set2')\n",
    "\n",
    "plt.title('Distribution of Emotions by Gender', size=16)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "plt.ylabel('Percent, %', size=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "sns.despine() # Remove the top and right spines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9601592",
   "metadata": {},
   "source": [
    "The distribution of emotions in the entire dataset by the gender is relatively uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811336b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of each emotion by gender\n",
    "emotions_by_gender = merged_df.groupby('audio_intensity')['emotions'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Reset the index to convert the result into a DataFrame\n",
    "emotions_by_gender = emotions_by_gender.reset_index(name='percentage')\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "sns.barplot(emotions_by_gender, x='emotions', y='percentage', hue='audio_intensity', hue_order=['high', 'normal', 'low'], palette='Set2')\n",
    "\n",
    "plt.title('Distribution of Emotions by Audio Intensity', size=16)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "plt.ylabel('Percent, %', size=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "sns.despine() # Remove the top and right spines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf57ee",
   "metadata": {},
   "source": [
    "The audio intensity distribution is divided by high, normal, and low. Neutral and surprised tones have only two intensities, normal and high. <br>\n",
    "The total distribution is uniform between all the modes of the tone intensity (15%-20%), where only in neutral tone there is a higher percentage of normal intensity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97073800-0471-4a2e-b9ed-61a799f223cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a histogram using seaborn\n",
    "sns.histplot(data=merged_df, x='duration', hue='dataset')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Sound Durations')\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d6d7a-73d1-4fba-b4c5-429133ad0feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"max input audion duration is {round(merged_df['duration'].max(),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38409777-9555-46fa-bf1b-b1a62ca8a0ba",
   "metadata": {},
   "source": [
    "The audio duration distribution of each dataset is slightly different, with the SAVEE dataset having the biggest spread.\n",
    "This means that as part of the data preparation for the model, we'll need to pad the audio files with zeroes based on the longest audio files in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2694ecc",
   "metadata": {},
   "source": [
    "## 3.2. Demonstration of various audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd954beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_random_audio(dataframe, gender, emotion, intensity, preprocess=False):\n",
    "    # Filter the DataFrame based on the given criteria\n",
    "    filtered_df = dataframe[(dataframe['genders'] == gender) & (dataframe['emotions'] == emotion) & (dataframe['audio_intensity'] == intensity)]\n",
    "    \n",
    "    # Randomly select an audio file path\n",
    "    file_path = random.choice(np.array(filtered_df['audio_path']))\n",
    "    \n",
    "    # Load the audio file using librosa\n",
    "    audio, sr = librosa.load(file_path)\n",
    "\n",
    "    # Applying preprocess\n",
    "    if preprocess:\n",
    "        audio = preprocess_audio(audio)\n",
    "        \n",
    "    # Display the waveform plot\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Waveform plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    librosa.display.waveshow(audio, sr=sr)\n",
    "    plt.title(f'Waveform')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "\n",
    "    # Spectrogram plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    img = librosa.display.specshow(spectrogram_db, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.title('Spectrogram')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Mel frequency')\n",
    "    plt.colorbar(format='%+2.0f dB')  # A colorbar with dB format\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.4)  #  Horizontal spacing between subplots\n",
    "    plt.suptitle(f'Dataset: {dataframe[dataframe[\"audio_path\"]==file_path][\"dataset\"].values[0]}, Gender: {gender}, Emotion: {emotion}, Intensity: {intensity}', weight='bold', size=16)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Play the audio\n",
    "    return ipd.Audio(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef2c23",
   "metadata": {},
   "source": [
    "Examples of random audio samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c229d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_random_audio(merged_df, gender=\"female\", emotion=\"fear\", intensity=\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c37840",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_random_audio(merged_df, gender=\"female\", emotion=\"happy\", intensity=\"low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_random_audio(merged_df, gender=\"male\", emotion=\"angry\", intensity=\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_random_audio(merged_df, gender=\"male\", emotion=\"disgust\", intensity=\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_random_audio(merged_df, gender=\"female\", emotion=\"surprised\", intensity=\"high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df24dba",
   "metadata": {},
   "source": [
    "Each of the imported datasets originated from a different location, i.e., some with background noise, different sentences, gender of speakers, etc... <br>\n",
    "This contributes to the quality of the whole data frame and thereafter to the ability of the model to classify an emotion of an unseen dataset. <br>\n",
    "From the samples above, one can qualitatively distinguish between the various emotions, despite being recorded in different environments (e.g., a studio or with a sort of environmental noise) or if the speaker says a different sentence. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb4d46",
   "metadata": {},
   "source": [
    "The Mel-frequency spectrogram is designed to better approximate the perception of sound by humans, especially in terms of the frequency resolution. It captures more details in the lower frequencies where human hearing is more sensitive and provides a more perceptually relevant representation compared to the regular spectrogram with a linear frequency scale.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66dc0d",
   "metadata": {},
   "source": [
    "# 4. Feature Extraction & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e3ac3e-0a72-4638-bfa4-3256e72ff7a0",
   "metadata": {},
   "source": [
    "## 4.1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1dfaf-8643-4250-a32e-da67a80d7aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_audio(audio):\n",
    "    \n",
    "    # waveform final samples length set to 160000 (~7.2 seconds) for SAVEE dataset compatability\n",
    "    samples_count = 160000\n",
    "    \n",
    "    # maximum decibles considered silence to be removed from start and end of audio\n",
    "    silence_db = 25\n",
    "    \n",
    "    trimmed_audio, _ = librosa.effects.trim(audio, top_db=silence_db)\n",
    "    padded_trimmed_audio = np.pad(trimmed_audio, (0, samples_count-len(trimmed_audio)), 'constant')\n",
    "    \n",
    "    return padded_trimmed_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd36d4-215a-44ba-a177-498e4d5c48ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "play_random_audio(merged_df, gender=\"male\", emotion=\"disgust\", intensity=\"high\", preprocess=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064a7255",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799817bf",
   "metadata": {},
   "source": [
    "## 2D Convolution on Mel-spectrum images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCDataCreator:\n",
    "    def __init__(self, num_mfcc=30):\n",
    "        self.num_mfcc = num_mfcc\n",
    "        self.max_length = 0\n",
    "\n",
    "    def create_data(self, df):\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            audio_path = row['audio_path']\n",
    "            emotion = row['emotions']\n",
    "            mfcc = self._extract_mfcc(audio_path)\n",
    "            X.append(mfcc)\n",
    "            y.append(emotion)\n",
    "            # Update maximum length if needed\n",
    "            if mfcc.shape[1] > self.max_length:\n",
    "                self.max_length = mfcc.shape[1]\n",
    "\n",
    "        # Apply padding or truncation with the computed maximum length\n",
    "        for i in range(len(X)):\n",
    "            if X[i].shape[1] < self.max_length:\n",
    "                pad_width = self.max_length - X[i].shape[1]\n",
    "                X[i] = np.pad(X[i], pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "            else:\n",
    "                X[i] = X[i][:, :self.max_length]\n",
    "\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        return X, y, self.max_length\n",
    "    \n",
    "    # extract_mfcc function is an internal method\n",
    "    def _extract_mfcc(self, audio_path):\n",
    "        audio, sr = librosa.load(audio_path)\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=self.num_mfcc)\n",
    "        return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d34482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history, metric):\n",
    "    train_metric = history.history[metric]\n",
    "    val_metric = history.history['val_' + metric]\n",
    "    \n",
    "    plt.plot(train_metric)\n",
    "    plt.plot(val_metric)\n",
    "    plt.title('Model ' + metric)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b72f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_early_stopping_callback():\n",
    "    return EarlyStopping(patience=5, monitor=\"val_loss\", mode='min', verbose=1)\n",
    "\n",
    "def create_model_checkpoint_callback(file_name):\n",
    "    return ModelCheckpoint(file_name, save_best_only=True, monitor=\"val_accuracy\")\n",
    "\n",
    "def create_reduce_lr_callback():\n",
    "    return ReduceLROnPlateau(factor=0.1, patience=3, monitor=\"val_accuracy\", min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add266c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, X, y, dataframe):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.dataframe = dataframe\n",
    "        self.num_classes = len(dataframe['emotions'].unique())\n",
    "        self.class_labels = list(dataframe['emotions'].unique())\n",
    "        self.label_encoder = LabelEncoder()\n",
    "    \n",
    "    def split_data(self, test_size=0.2, val_size=0.1, random_state=42):\n",
    "        # This methods splits the arrays after the mfcc extraction\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "            self.X, self.y, test_size=test_size, shuffle=True, random_state=random_state)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val, test_size=val_size, random_state=random_state)\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \n",
    "    def split_dataframe(self, test_size=0.2, val_size=0.1, random_state=42):\n",
    "        # Splitting the data into train, val, test datasets\n",
    "        train_val, test = train_test_split(self.dataframe, test_size=test_size, shuffle=True, random_state=random_state)\n",
    "        train, val = train_test_split(train_val, test_size=val_size, shuffle=True, random_state=random_state)\n",
    "\n",
    "        train = train.reset_index(drop=True)\n",
    "        val = val.reset_index(drop=True)\n",
    "        test = test.reset_index(drop=True)\n",
    "        return train, val, test\n",
    "    \n",
    "    def normalize_data(self, X_train, X):\n",
    "        mean = np.mean(X_train, axis=0)\n",
    "        std = np.std(X_train, axis=0)\n",
    "        X_scaled = (X - mean) / std\n",
    "        return X_scaled\n",
    "    \n",
    "    def modify_channels(self, X):\n",
    "        X_modified = X[..., np.newaxis]\n",
    "        return X_modified\n",
    "    \n",
    "    def encode_labels(self, y_train, y_val, y_test):\n",
    "        y_train_encoded = self.label_encoder.fit_transform(y_train)\n",
    "        y_val_encoded = self.label_encoder.transform(y_val)\n",
    "        y_test_encoded = self.label_encoder.transform(y_test)\n",
    "        return y_train_encoded, y_val_encoded, y_test_encoded\n",
    "    \n",
    "    def one_hot_encode_labels(self, y_encoded):\n",
    "        y_categorical = to_categorical(y_encoded, self.num_classes, dtype='float32')\n",
    "        return y_categorical\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = self.split_data()\n",
    "        X_train_scaled = self.normalize_data(X_train, X_train)\n",
    "        X_val_scaled = self.normalize_data(X_train, X_val)\n",
    "        X_test_scaled = self.normalize_data(X_train, X_test)\n",
    "        X_train_modified = self.modify_channels(X_train_scaled)\n",
    "        X_val_modified = self.modify_channels(X_val_scaled)\n",
    "        X_test_modified = self.modify_channels(X_test_scaled)\n",
    "        y_train_encoded, y_val_encoded, y_test_encoded = self.encode_labels(y_train, y_val, y_test)\n",
    "        y_train_categorical = self.one_hot_encode_labels(y_train_encoded)\n",
    "        y_val_categorical = self.one_hot_encode_labels(y_val_encoded)\n",
    "        y_test_categorical = self.one_hot_encode_labels(y_test_encoded)\n",
    "        \n",
    "        return X_train_modified, X_val_modified, X_test_modified, y_train_categorical, y_val_categorical, y_test_categorical, self.num_classes, self.class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ee0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier:\n",
    "    def __init__(self, model, class_labels, dataframe):\n",
    "        self.model = model\n",
    "        self.dataframe = dataframe\n",
    "        self.class_labels = class_labels\n",
    "    \n",
    "    def predict_sample(self, test):\n",
    "\n",
    "        # Randomly choose a sample from X_test_scaled\n",
    "        sample_index  = np.random.choice(self.dataframe.shape[0])\n",
    "        # Get the selected sample\n",
    "        sample = test[sample_index]\n",
    "\n",
    "        prediction = self.model.predict(np.expand_dims(sample, axis=0))\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        predicted_emotion = self.class_labels[predicted_class]\n",
    "        \n",
    "        # Display sample and prediction\n",
    "        print(\"Sample Emotion:\", self.dataframe.iloc[sample_index].emotions)\n",
    "        print(\"Predicted Emotion:\", predicted_emotion)\n",
    "        \n",
    "        # Display sound \n",
    "        audio = ipd.Audio(self.dataframe.iloc[sample_index].audio_path)\n",
    "        display(audio)\n",
    "        \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        loss, accuracy = self.model.evaluate(X_test, y_test)\n",
    "        print(\"Test Loss:\", loss)\n",
    "        print(\"Test Accuracy:\", accuracy)\n",
    "        \n",
    "    def plot_confusion_matrix(self, X_test, y_test, class_labels):\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Convert y_pred from label-encoded to one-hot encoded format\n",
    "        y_pred_onehot = to_categorical(y_pred, num_classes)\n",
    "\n",
    "        # Decode y_pred and y_test back to their original class labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(class_labels)\n",
    "        y_pred_labels = label_encoder.inverse_transform(np.argmax(y_pred_onehot, axis=1))\n",
    "        y_test_labels = label_encoder.inverse_transform(np.argmax(y_test, axis=1))\n",
    " \n",
    "        cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "        plt.xlabel(\"Predicted Emotion\")\n",
    "        plt.ylabel(\"True Emotion\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate row sums for percentage calculation\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # Calculate confusion matrix in percentage format\n",
    "        cm_percentage = (cm / row_sums) * 100\n",
    "\n",
    "        # Plot the confusion matrix in percentage format\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm_percentage, annot=True, fmt=\".0f\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "        plt.xlabel(\"Predicted Labels\")\n",
    "        plt.ylabel(\"True Labels\")\n",
    "        plt.title(\"Confusion Matrix (Percentage)\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_metrics(self, history):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cfcf84-9a71-44f5-be04-d4f010472cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_creator = MFCCDataCreator(num_mfcc=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8873401-7492-4074-8c2f-1a31b50bd4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, max_length = data_creator.create_data(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7dbfd8-daaa-4fff-8ccd-6d71af6f2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = DataPreprocessor(X, y, merged_df)\n",
    "X_train_scaled, X_val_scaled, X_test_scaled, y_train_categorical, y_val_categorical, y_test_categorical, num_classes, class_labels = preprocessor.preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854448fe-25ac-4906-bc4d-2f4071183fb6",
   "metadata": {},
   "source": [
    "### 3x3 kernel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf72d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train_scaled.shape[1:]\n",
    "\n",
    "# Define the optimizer with an initial learning rate\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd15786",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    create_early_stopping_callback(),\n",
    "    create_model_checkpoint_callback(\"best_model_kernel3by3.h5\"),\n",
    "    create_reduce_lr_callback()\n",
    "]\n",
    "\n",
    "history = model.fit(X_train_scaled, \n",
    "                    y_train_categorical, \n",
    "                    batch_size=32, \n",
    "                    epochs=30, \n",
    "                    validation_data=(X_val_scaled, y_val_categorical),\n",
    "                    callbacks=callbacks\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c24e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history, 'accuracy')\n",
    "plot_metrics(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b001931d-ef42-4e39-9334-f503270e3963",
   "metadata": {},
   "source": [
    "### 4x10 kernel model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10df062-7811-4287-b23b-931fdd7ba204",
   "metadata": {},
   "source": [
    "Since the MFCC shape is of a wide rectangle, a rectangular kernel might be more fitted for our model Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa540c-0d9a-4b5c-9e3e-0acc19e9d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train_scaled.shape[1:]\n",
    "\n",
    "# Define the optimizer with an initial learning rate\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv2D(32, kernel_size=(4, 10), activation=\"relu\", padding=\"same\")(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Conv2D(32, kernel_size=(4, 10), activation=\"relu\", padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Conv2D(64, kernel_size=(4, 10), activation=\"relu\", padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Conv2D(64, kernel_size=(4, 10), activation=\"relu\", padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de91bee-b8d3-4fff-82f6-c09abe9db6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    create_early_stopping_callback(),\n",
    "    create_model_checkpoint_callback(\"best_model_kernel4by10.h5\"),\n",
    "    create_reduce_lr_callback()\n",
    "]\n",
    "\n",
    "history = model.fit(X_train_scaled, \n",
    "                    y_train_categorical, \n",
    "                    batch_size=32, \n",
    "                    epochs=30, \n",
    "                    validation_data=(X_val_scaled, y_val_categorical),\n",
    "                    callbacks=callbacks\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0bce34-de4a-46f7-888c-a7583076d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history, 'accuracy')\n",
    "plot_metrics(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4d4b1",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd71468-9554-4056-92f1-81a4a12b5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentation:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def shift_pitch(self, audio, sr, semitones):\n",
    "        shifted_audio = librosa.effects.pitch_shift(audio, n_steps=float(semitones), sr=sr)\n",
    "        return shifted_audio\n",
    "    \n",
    "    def time_shift(self, audio, sr, shift_range=5):\n",
    "        shift_samples = np.random.randint(-shift_range, shift_range)*1000\n",
    "        shifted_audio = np.roll(audio, shift=shift_samples)\n",
    "        return shifted_audio\n",
    "    \n",
    "    def time_stretch(self, audio, rate):\n",
    "        stretched_audio = librosa.effects.time_stretch(audio, rate=rate)\n",
    "        return stretched_audio\n",
    "    \n",
    "    def add_noise(self, audio, noise_factor):\n",
    "        noise = np.random.randn(len(audio))\n",
    "        noise_factor = noise_factor *np.random.uniform() * np.amax(audio)\n",
    "        noisy_audio = audio + noise_factor * noise\n",
    "        return noisy_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4742c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = DataAugmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1fcf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random 75% of the data\n",
    "random_merged_df = merged_df.sample(frac=0.75, random_state=42)\n",
    "random_merged_df.reset_index(drop=True, inplace=True)\n",
    "random_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c103157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe to store augmented data\n",
    "augmented_df = pd.DataFrame(columns=random_merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set output folder to save augmented audio files\n",
    "output_folder = './data/augmented_audio'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad181640",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in random_merged_df.iterrows():\n",
    "    audio_path = row['audio_path']\n",
    "    emotion = row['emotions']\n",
    "    \n",
    "    # Load the audio\n",
    "    audio, sr = librosa.load(audio_path, sr=None)\n",
    "    \n",
    "    # Augment the audio using each method in the DataAugmentation class\n",
    "    shifted_audio = augmentor.shift_pitch(audio, sr, semitones=2)\n",
    "    time_shifted_audio = augmentor.time_shift(audio, sr, shift_range=5)\n",
    "    stretched_audio = augmentor.time_stretch(audio, rate=1.2)\n",
    "    noisy_audio = augmentor.add_noise(audio, noise_factor=0.1)\n",
    "    \n",
    "    # Save the augmented audio if needed\n",
    "    output_path = os.path.join(output_folder, f'{emotion}_{idx}.wav')\n",
    "    sf.write(output_path, shifted_audio, sr)\n",
    "    \n",
    "    # Append the augmented data to the augmented_df\n",
    "    augmented_df = pd.concat([augmented_df, \n",
    "                              pd.DataFrame([{\n",
    "                                  'emotions': emotion,\n",
    "                                  'genders': row['genders'],\n",
    "                                  'audio_intensity': row['audio_intensity'],\n",
    "                                  'audio_path': output_path,  # Replace with the actual path if not saving\n",
    "                                  'duration': row['duration'],\n",
    "                                  'dataset': row['dataset']\n",
    "                              }])])\n",
    "\n",
    "augmented_df.to_csv('augmented_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794e1fd-da0c-4a86-955e-fbcd75ac1619",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('merged_df.csv')\n",
    "augmented_df = pd.read_csv('augmented_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4037c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.concat([merged_df, augmented_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c673d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_creator = MFCCDataCreator(num_mfcc=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72cbdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=final_data, x='duration', hue='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3282c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301971c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, max_length = data_creator.create_data(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eba653",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = DataPreprocessor(X, y, final_data)\n",
    "X_train_scaled, X_val_scaled, X_test_scaled, y_train_categorical, y_val_categorical, y_test_categorical, num_classes, class_labels = preprocessor.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a08d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train , val, test datasets\n",
    "_, _, test = preprocessor.split_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ce1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train_scaled.shape[1:]\n",
    "\n",
    "# Define the optimizer with an initial learning rate\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    create_early_stopping_callback(),\n",
    "    create_model_checkpoint_callback(\"best_model_kernel3by3_augmentation.h5\"),\n",
    "    create_reduce_lr_callback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b5153",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_scaled, \n",
    "                    y_train_categorical, \n",
    "                    batch_size=32, \n",
    "                    epochs=50, \n",
    "                    validation_data=(X_val_scaled, y_val_categorical),\n",
    "                    callbacks=callbacks\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d4f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_classifier = EmotionClassifier(model, class_labels, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f6ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_classifier.predict_sample(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_classifier.evaluate(X_test_scaled, y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6503d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_classifier.plot_confusion_matrix(X_test_scaled, y_test_categorical, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_classifier.plot_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce6ec3",
   "metadata": {},
   "source": [
    "## Transfer learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c9caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
